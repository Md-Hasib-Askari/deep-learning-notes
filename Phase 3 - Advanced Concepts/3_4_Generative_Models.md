
## ğŸ“˜ 3.4: **Generative Models** (VAEs + GANs)

### ğŸ¯ Goal

Understand how models can learn to **generate new data** â€” like images, audio, or text â€” that resemble training samples.

---

## ğŸ§  What Are Generative Models?

Generative models learn to capture the **underlying distribution** of data and can **sample new data points** from it.

### ğŸ”„ Contrast with Discriminative Models:

| Discriminative               | Generative               |                           |        |
| ---------------------------- | ------------------------ | ------------------------- | ------ |
| Learn \*\*P(y                | x)\*\*                   | Learn **P(x)** or \*\*P(x | y)\*\* |
| Focus on classification      | Focus on data generation |                           |        |
| Example: Logistic Regression | Example: GAN, VAE        |                           |        |

---

## ğŸ”® 1. **Variational Autoencoders (VAEs)**

### ğŸ”¹ Structure

* **Encoder**: Maps input â†’ latent space (mean + std)
* **Latent space**: A distribution (not a point)
* **Decoder**: Reconstructs data from latent vector

### ğŸ§  Why "Variational"?

It learns a probability **distribution** over latent variables rather than fixed encodings.

### ğŸ” Key Loss Function:

$$
\text{Loss} = \text{Reconstruction Loss} + \text{KL Divergence}
$$

### ğŸ”§ PyTorch Sketch:

```python
z = mu + sigma * torch.randn_like(sigma)  # Reparameterization trick
```

### âœ… Use Cases

* Denoising images
* Anomaly detection
* Generative sampling (less sharp than GANs)

---

## ğŸ§¨ 2. **Generative Adversarial Networks (GANs)**

### ğŸ”¹ Core Idea: A two-player game

| Component         | Role                              |
| ----------------- | --------------------------------- |
| **Generator**     | Tries to generate fake data       |
| **Discriminator** | Tries to distinguish real vs fake |

They compete, and both improve.

### ğŸ¯ Objective:

$$
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

### ğŸ”§ GAN Training Pitfalls:

* Mode collapse
* Training instability
* Needs careful tuning (learning rate, architecture)

### âœ… Use Cases

* Image synthesis (e.g., StyleGAN)
* Super-resolution
* Image-to-image translation (Pix2Pix, CycleGAN)

---

## ğŸ“Š Comparison: VAE vs GAN

| Feature        | VAE                       | GAN                      |
| -------------- | ------------------------- | ------------------------ |
| Stability      | More stable training      | Can be unstable          |
| Output Quality | Blurry                    | Sharp                    |
| Latent Space   | Structured, interpretable | Often less interpretable |
| Use Case       | Encoding + generation     | Pure generation          |

---

## ğŸ§ª Exercises

### âœ… Conceptual

1. Why do GANs need a discriminator?
2. What does KL divergence enforce in VAEs?

### âœ… Practical

* Train a VAE on MNIST and visualize the latent space.
* Train a simple GAN on Fashion-MNIST.
* Modify a GAN to generate hand-written digits from noise.

